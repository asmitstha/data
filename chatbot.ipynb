{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "chatbot",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNgFzqjmTmmd",
        "colab_type": "code",
        "outputId": "be423277-425a-4e4b-812c-6f3aa4596f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers , activations , models , preprocessing\n",
        "\n",
        "print( tf.VERSION )\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v1_FFbQUzia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75b096a9-9c8e-4da2-8986-38c1b90c1271"
      },
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get( 'https://github.com/asmitstha/data/blob/master/data.zip?raw=true' ) \n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "print(z)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<zipfile.ZipFile file=<_io.BytesIO object at 0x7f48d1f7b830> mode='r'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7AtxF3hU0jO",
        "colab_type": "code",
        "outputId": "205398b0-5943-480f-a820-0bafa028166e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "dir_path = r'C:/Users/asmit/Downloads/Chatbot/fyp/chat/data'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0ac355a681b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'C:/Users/asmit/Downloads/Chatbot/fyp/chat/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/asmit/Downloads/Chatbot/fyp/chat/data/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFjOMyYJTmni",
        "colab_type": "code",
        "outputId": "fb1884dd-0cd9-427f-fabb-3fbee4b309ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(answers)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "564"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "CfGMCDkHTmnp",
        "colab_type": "code",
        "outputId": "78a55415-cced-48dd-e758-41f1fc88c435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import string\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )\n",
        "\n",
        "def tokenize( sentences ):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = str(sentence).lower()\n",
        "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
        "        result = sentence.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "        tokens = result.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append( tokens )\n",
        "    return tokens_list , vocabulary\n",
        "\n",
        "p = tokenize( questions + answers )\n",
        "len(p)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv1a90uPTmny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = Word2Vec( p[ 0 ], min_count=0 ) \n",
        "# print(model)\n",
        "\n",
        "# embedding_matrix = np.zeros( ( VOCAB_SIZE , 100 ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVq5kH7sTmn6",
        "colab_type": "code",
        "outputId": "18c26612-3b45-4324-bf3f-84d96be5775f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions ) #Transform each question in a sequence of integers.\n",
        "maxlen_questions = max( [ len(x) for x in tokenized_questions ] ) # question with maximum length i.e is 50\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' ) #ensure that all sequences in a list have the same length\n",
        "encoder_input_data = np.array( padded_questions ) #create array\n",
        "print( encoder_input_data.shape , maxlen_questions )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(564, 22) 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZZ2VdDgTmoH",
        "colab_type": "code",
        "outputId": "55d23df3-eccd-44ab-8a38-beff34724037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers ) #Transform each text in texts in a sequence of integers.\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] ) # answer with maximum length i.e is 74\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )  #ensure that all sequences in a list have the same length\n",
        "decoder_input_data = np.array( padded_answers ) #create array\n",
        "print( decoder_input_data.shape , maxlen_answers )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(564, 74) 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H716GoYuTmoT",
        "colab_type": "code",
        "outputId": "6a4ef027-08b3-4d5e-d0d9-14c5b1f4b1c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# decoder_output_data: Tokenize the answers. Remove the first element from all the tokenized_answers. \n",
        "#This is the <START> element which we added earlier.\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers ) #Transform each text in texts in a sequence of integers.\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:] #remove first element form all the tokenized_answers.\n",
        "    \n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )  #ensure that all sequences in a list have the same length\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE ) #Converts a class vector (integers) to binary class matrix.\n",
        "decoder_output_data = np.array( onehot_answers ) #create array of onehot_answers\n",
        "\n",
        "print( decoder_output_data.shape )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(564, 74, 1894)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99jKBDl0Tmoc",
        "colab_type": "code",
        "outputId": "a77ac8d3-c7da-494f-e2cd-1862bfd115cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( None , )) #Input() is used to instantiate a Keras tensor\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs) #Turns positive integers (indexes) into dense vectors of fixed size\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 200)    378800      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    378800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 1894)   380694      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,779,894\n",
            "Trainable params: 1,779,894\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_YuAq7qTmok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd4Qo-mcTmo2",
        "colab_type": "code",
        "outputId": "9d5e725d-f4a1-4bc8-d27c-7fee2ae2710d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=300 ) \n",
        "\n",
        "model.save(\"last_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 564 samples\n",
            "Epoch 1/300\n",
            "564/564 [==============================] - 13s 22ms/sample - loss: 1.3043\n",
            "Epoch 2/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.1152\n",
            "Epoch 3/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.0923\n",
            "Epoch 4/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.0740\n",
            "Epoch 5/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.0585\n",
            "Epoch 6/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.0419\n",
            "Epoch 7/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.0274\n",
            "Epoch 8/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 1.0137\n",
            "Epoch 9/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.9989\n",
            "Epoch 10/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.9837\n",
            "Epoch 11/300\n",
            "564/564 [==============================] - 3s 5ms/sample - loss: 0.9680\n",
            "Epoch 12/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.9506\n",
            "Epoch 13/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.9343\n",
            "Epoch 14/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.9183\n",
            "Epoch 15/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.9036\n",
            "Epoch 16/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8868\n",
            "Epoch 17/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8721\n",
            "Epoch 18/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8572\n",
            "Epoch 19/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8438\n",
            "Epoch 20/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8288\n",
            "Epoch 21/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8156\n",
            "Epoch 22/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.8025\n",
            "Epoch 23/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7890\n",
            "Epoch 24/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7766\n",
            "Epoch 25/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7632\n",
            "Epoch 26/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7511\n",
            "Epoch 27/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7385\n",
            "Epoch 28/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7277\n",
            "Epoch 29/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7159\n",
            "Epoch 30/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.7032\n",
            "Epoch 31/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6923\n",
            "Epoch 32/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6821\n",
            "Epoch 33/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6702\n",
            "Epoch 34/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6583\n",
            "Epoch 35/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6484\n",
            "Epoch 36/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6381\n",
            "Epoch 37/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6267\n",
            "Epoch 38/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6156\n",
            "Epoch 39/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.6060\n",
            "Epoch 40/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5950\n",
            "Epoch 41/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5858\n",
            "Epoch 42/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5752\n",
            "Epoch 43/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5652\n",
            "Epoch 44/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5546\n",
            "Epoch 45/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5433\n",
            "Epoch 46/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5357\n",
            "Epoch 47/300\n",
            "564/564 [==============================] - 2s 4ms/sample - loss: 0.5244\n",
            "Epoch 48/300\n",
            "350/564 [=================>............] - ETA: 0s - loss: 0.4848"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1fc17358348d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UREt3xFqTmpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# model.load_weights('new_model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJZ4ckE7Tmpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNt3acqtTmpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NGm0PK1Tmp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save(\"new_model.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUena9NSTmqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.models import load_model\n",
        "# new_model=load_model('new_model.h5')\n",
        "# enc_new_model=load_model('encoder_model.h5')\n",
        "# dec_new_model=load_model('decoder_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEFubkxETmqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf \n",
        "# new_model = tf.keras.models.load_model('new_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYztK70MTmqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIr-iAYkTmq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObJ_VVUNTmq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "   \n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_embedding , initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = tf.keras.models.Model( [decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imLn6gJOTmrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# enc_new_model = tf.keras.models.load_model('encoder_model.h5')\n",
        "# enc_new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtqJXOHzTmrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dec_new_model = tf.keras.models.load_model('decoder_model.h5')\n",
        "# dec_new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7lxsESETmrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# enc_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_wilXsNTmrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    words = sentence.translate(str.maketrans(\"\",\"\", string.punctuation)).split()\n",
        "    print(words)\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQRlVac1Tmrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str_to_tokens(\"how\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByoSK7QCTmrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU4KWP6uTmr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = encoder_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation ='BOT : '+ ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = decoder_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHz1U1J5Tmr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8geyfuR0TmsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3YS7WnMTmsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}